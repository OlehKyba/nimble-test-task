# Тестовое задания для позиции Junior Python Developer в компании Nimble Inc.
## Задание:
Cпроектировать и написать отказоустойчивый и масштабируемый REST-сервис для хранения бинарных данных с любым подходящим облачным сервисом (S3, Azure Blob Storage) в роли фактического хранилища данных. Доступ к данным осуществляется по ключу (чистый key-value).
## Требования к сервису:
* операции put, get, доступные через REST
* синхронная запись (данные доступны для get сразу же после завершения put)
* возможность горизонтального масштабирования добавлением новых серверов
* не использовать Django
## Анализ требований к сервису
### Операции put, get, доступные через REST:
В моем сервесе доступны такие маршруты:
 / | /put/ | /get/<key:string>
-|------------ | -------------
Методы | POST | GET 
Принимают | JSON FILE (описан ниже) | key - ключ, по которому храняться данные
Возвращают | JSON PUT RESPONSE | JSON FILE (описан ниже) 

**JSON FILE** формат:
```json
{
    "key": "Ключ, по которому храняться данные",
    "extension": "Расширение файла",
    "data": "Байты в кодировке base64, сериализованые в строку"
}
```
**JSON PUT RESPONSE** формат:
```json
{
    "msg": "Сообщение"
}
```  
### Cинхронная запись:
Во время записи, чтобы пользователь не ждал окончания запроса в облачный сервис, бинарные данные сохраняються в кеш (Redis). После чего мы ставим задачу на запись данных в очередь (использовалась библиотек rq). Синхронность обеспечиваеться тем, что при попытки чтения сервис отдает данные из кеша.
### Возможность горизонтального масштабирования добавлением новых серверов:
В сервисе используються контейнеры для сервера(web), контейнеры для решения отложенных задач (worker), контейнер для кеша и брокер сообщений (redis), прокси-сервер (nginx, он же и делает балансировку между контейнерами-серверами по алгоритму Round Robin).
Чтобы горизонтально маштабировать сервис, достаточно добавить новые узлы в Kubernetes, Docker Swarm или ECS. В этом проекте все подготовлено для использования вместе с Docker Swarm (из-за его простоты).

Схема из файла docker-compouse-swarm.yml для 4 узлов:

![Схема из 4 узлов по 1 контейнеру](https://drive.google.com/uc?export=view&id=1xyNcBLd-Xkxu2otEXydt5LMkBR0AcMO9)

Чтобы маштабировать количество контейнеров web и worker, например до 3, достаточно:

```bash
olehkyba@iGood:~/Projects/test_task/flask$ docker service scale flask_web=3
flask_web scaled to 3
overall progress: 3 out of 3 tasks 
1/3: running   [==================================================>] 
2/3: running   [==================================================>] 
3/3: running   [==================================================>] 
verify: Service converged 
olehkyba@iGood:~/Projects/test_task/flask$ docker service scale flask_worker=3
flask_worker scaled to 3
overall progress: 3 out of 3 tasks 
1/3: running   [==================================================>] 
2/3: running   [==================================================>] 
3/3: running   [==================================================>] 
verify: Service converge
``` 
![Обновленная схема](https://drive.google.com/uc?export=view&id=1LMD_jN5QYEQdILd-5ie8xXKVvKsPisr5)

Также есть возможность горизантального масштабирования Redis c помощью Redis Cluster.
### Не использовать Django:
При написании сервиса использовался другой популярный фреймворк - Flask.
## Как запустить?
Достаточно скопировать репозиторий и выполнить команду внутри него:
```bash
olehkyba@iGood:~/Projects/test_task/flask$ docker-compose up
``` 
## Что можно улучшить?
* Это приложение - прокси между облачным сервисом и клиентом. Для задач, в которых основные ресурсы тратяться на сетевое взаимодействие, лучше использовать асинхронный фреймворк/библиотеку (Tornado, aiohttp, FastAPI, starlette). Вероятно, что эти решения были бы более производительные, чем Flask.
* Я старался придерживаться DDD (Domain-driven design) в архитектуре этого проекта, но мне не нравится, как выглядят юзкейсы. Я думаю, чтобы их исправить, нужно прибегнуть к Event Driven Architecture и переписать юзкейсы на обработчики событий. Я этого не сделал, чтобы не усложнять код, вводя события и шину событий.
* В этом сервисе есть DI, но он сделан не самым лучшим образом, используя Root Component. Если сервис будет расти, нужно будет заменить такой DI на что-то более гибкое с нормальным IoC-Контейнером (например, библиотека injector).
